{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset from yolo's dataset"
      ],
      "metadata": {
        "id": "-kwFVtMVUbs_"
      },
      "id": "-kwFVtMVUbs_"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/egg detection label.v1i.yolov8.zip\" -d \"/content/egg-detection-label-yolov8\""
      ],
      "metadata": {
        "id": "wDcnsV_rQhr9"
      },
      "id": "wDcnsV_rQhr9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c3151f2-f527-4e18-8395-cb4882191875",
      "metadata": {
        "id": "6c3151f2-f527-4e18-8395-cb4882191875"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "dataset_path = \"/content/egg-detection-label-yolov8\"\n",
        "image_dir = \"/content/egg-detection-label-yolov8/train/images\"\n",
        "labels_dir = \"/content/egg-detection-label-yolov8/train/labels\"\n",
        "output_dir = \"/content/egg-cropped\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for label_file in os.listdir(labels_dir):\n",
        "  if not label_file.endswith(\".txt\"):\n",
        "    continue\n",
        "\n",
        "  image_name = label_file.replace(\".txt\", \".jpg\")\n",
        "  image_path = os.path.join(image_dir, image_name)\n",
        "\n",
        "  if not os.path.exists(image_path):\n",
        "    continue\n",
        "\n",
        "  img = cv2.imread(image_path)\n",
        "  h, w, _ = img.shape\n",
        "\n",
        "  with open(os.path.join(labels_dir, label_file), \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "      class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "\n",
        "      x_center *= w\n",
        "      y_center *= h\n",
        "      width *= w\n",
        "      height *= h\n",
        "\n",
        "      x1 = int(x_center - width / 2)\n",
        "      y1 = int(y_center - height / 2)\n",
        "      x2 = int(x_center + width / 2)\n",
        "      y2 = int(y_center + height / 2)\n",
        "\n",
        "      crop = img[y1:y2, x1:x2]\n",
        "\n",
        "      if crop.size == 0:\n",
        "        continue\n",
        "\n",
        "      # create folder\n",
        "      class_folder = os.path.join(output_dir, f\"class{int(class_id)}\")\n",
        "      os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "      # save img\n",
        "      output_path = os.path.join(class_folder, f\"{image_name[:-4]}_{i}.jpg\")\n",
        "      cv2.imwrite(output_path, crop)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"/content/egg-cropped/class0\" \"/content/egg-cropped/Damaged\"\n",
        "!mv \"/content/egg-cropped/class1\" \"/content/egg-cropped/Not Damaged\""
      ],
      "metadata": {
        "id": "2i3G7K8TT49L"
      },
      "id": "2i3G7K8TT49L",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = '/content/egg-cropped'\n",
        "\n",
        "classes = os.listdir(dataset_dir)\n",
        "print(f\"classes: {classes}\")\n",
        "\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(dataset_dir, cls)\n",
        "    print(f\"{cls}: {len(os.listdir(cls_path))} images\")"
      ],
      "metadata": {
        "id": "ynjACpsXU4_p",
        "outputId": "de4b7874-ad13-43b5-c270-e7a5b9da8ecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ynjACpsXU4_p",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: ['Damaged', 'Not Damaged']\n",
            "Damaged: 728 images\n",
            "Not Damaged: 177 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Stages model"
      ],
      "metadata": {
        "id": "up5dIItuTm5K"
      },
      "id": "up5dIItuTm5K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## classification model"
      ],
      "metadata": {
        "id": "GezSlniP7rSs"
      },
      "id": "GezSlniP7rSs"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_dir = \"/content/egg-cropped\"\n",
        "output_dir = \"/content/egg-split-cropped\"\n",
        "\n",
        "train_dir = os.path.join(output_dir, \"train\")\n",
        "val_dir = os.path.join(output_dir, \"val\")\n",
        "test_dir = os.path.join(output_dir, \"test\")\n",
        "\n",
        "for d in [train_dir, val_dir, test_dir]:\n",
        "  os.makedirs(d, exist_ok=True)\n",
        "\n",
        "val_size = 0.15\n",
        "test_size = 0.15\n",
        "\n",
        "for class_name in os.listdir(dataset_dir):\n",
        "  class_path = os.path.join(dataset_dir, class_name)\n",
        "  if not os.path.isdir(class_path):\n",
        "    continue\n",
        "\n",
        "  images = os.listdir(class_path)\n",
        "  images = [img for img in images if img.endswith(('.jpg'))]\n",
        "\n",
        "  train_imgs, temp_imgs = train_test_split(images, test_size=(val_size + test_size), random_state=42)\n",
        "  val_imgs, test_imgs = train_test_split(temp_imgs,test_size=test_size / (val_size + test_size), random_state=42)\n",
        "\n",
        "  # copy files\n",
        "  for split, split_imgs in zip(\n",
        "      [train_dir, val_dir, test_dir],\n",
        "      [train_imgs, val_imgs, test_imgs]\n",
        "  ):\n",
        "    split_class_dir = os.path.join(split, class_name)\n",
        "    os.makedirs(split_class_dir, exist_ok=True)\n",
        "    for img_name in split_imgs:\n",
        "      shutil.copy(os.path.join(class_path, img_name), os.path.join(split_class_dir, img_name))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9ujhLihInERd"
      },
      "id": "9ujhLihInERd",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/egg-split-cropped-new.zip' -d '/content/egg-split-cropped-new'"
      ],
      "metadata": {
        "id": "dp6WzN-r7_J4"
      },
      "id": "dp6WzN-r7_J4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "data_dir = '/content/egg-split-cropped-new/egg-split-cropped'\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(data_dir, \"val\"),\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(data_dir, \"test\"),\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "# normalizing and caching\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def preprocess(ds, training=False):\n",
        "    if training:\n",
        "        ds = ds.shuffle(1000)\n",
        "    return ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds = preprocess(train_ds)\n",
        "val_ds = preprocess(val_ds)\n",
        "test_ds = preprocess(test_ds)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40jakeJFoakN",
        "outputId": "7401435b-9729-46bd-c8b6-1981f49f1653"
      },
      "id": "40jakeJFoakN",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 600 files belonging to 2 classes.\n",
            "Found 132 files belonging to 2 classes.\n",
            "Found 134 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "base_model.trainable = False  # freeze base\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    data_augmentation,\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMfkp7gTw2Ky",
        "outputId": "a31b97e1-a209-4f93-a3ae-3505f3bb0cc3"
      },
      "id": "KMfkp7gTw2Ky",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for c in os.listdir('/content/egg-split-cropped-new/egg-split-cropped/train'):\n",
        "    print(c, len(os.listdir(os.path.join('/content/egg-split-cropped-new/egg-split-cropped/train', c))))\n"
      ],
      "metadata": {
        "id": "QwS8YMDHCBxE",
        "outputId": "7b0849e2-e71e-46ee-84fc-1f6722034b8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QwS8YMDHCBxE",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Damaged 491\n",
            "Not Damaged 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t2czlm8xFAp",
        "outputId": "98f1f2b6-9fca-4dca-bcf0-28e13b07f3b9"
      },
      "id": "1t2czlm8xFAp",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.8165 - loss: 0.4987 - val_accuracy: 0.8182 - val_loss: 0.4742\n",
            "Epoch 2/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.8206 - loss: 0.4992 - val_accuracy: 0.8182 - val_loss: 0.4742\n",
            "Epoch 3/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4808 - val_accuracy: 0.8182 - val_loss: 0.4746\n",
            "Epoch 4/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4761 - val_accuracy: 0.8182 - val_loss: 0.4741\n",
            "Epoch 5/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.8206 - loss: 0.4787 - val_accuracy: 0.8182 - val_loss: 0.4742\n",
            "Epoch 6/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4892 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 7/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.8206 - loss: 0.4803 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 8/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.8206 - loss: 0.4755 - val_accuracy: 0.8182 - val_loss: 0.4741\n",
            "Epoch 9/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.8206 - loss: 0.4797 - val_accuracy: 0.8182 - val_loss: 0.4742\n",
            "Epoch 10/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.8206 - loss: 0.4800 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 11/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.8206 - loss: 0.4717 - val_accuracy: 0.8182 - val_loss: 0.4741\n",
            "Epoch 12/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.8206 - loss: 0.4739 - val_accuracy: 0.8182 - val_loss: 0.4741\n",
            "Epoch 13/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4828 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 14/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4785 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 15/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4959 - val_accuracy: 0.8182 - val_loss: 0.4744\n",
            "Epoch 16/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.8206 - loss: 0.4773 - val_accuracy: 0.8182 - val_loss: 0.4741\n",
            "Epoch 17/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.8206 - loss: 0.4861 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 18/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.8206 - loss: 0.4812 - val_accuracy: 0.8182 - val_loss: 0.4743\n",
            "Epoch 19/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.8206 - loss: 0.4753 - val_accuracy: 0.8182 - val_loss: 0.4741\n",
            "Epoch 20/20\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8206 - loss: 0.4783 - val_accuracy: 0.8182 - val_loss: 0.4746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/egg-split-cropped.zip\" \"/content/egg-split-cropped\""
      ],
      "metadata": {
        "id": "5mHsWFpcy7Er"
      },
      "id": "5mHsWFpcy7Er",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "C2kaY2ZK3v8v",
        "outputId": "a2a86f79-b243-4ec2-d12f-040e45133fd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "C2kaY2ZK3v8v",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328ms/step - accuracy: 0.7972 - loss: 0.5045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.48135852813720703, 0.8134328126907349]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35U9l-yBBX0G"
      },
      "id": "35U9l-yBBX0G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}